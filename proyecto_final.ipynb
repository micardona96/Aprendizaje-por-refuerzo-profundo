{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNZSESXtGljg"
      },
      "source": [
        "![MAIA banner](https://raw.githubusercontent.com/MAIA4361-Aprendizaje-refuerzo-profundo/Notebooks_Tareas/main/Images/Aprendizaje_refuerzo_profundo_Banner_V1.png)\n",
        "\n",
        "# <h1><center>Reto 2 - Solución Completa</center></h1>\n",
        "\n",
        "<center><h1>Robótica con Gymnasium Robotics</h1></center>\n",
        "\n",
        "**Estudiante:** [Tu Nombre]\n",
        "**Fecha:** [Fecha]\n",
        "\n",
        "Este notebook contiene la solución completa para el Reto 2, implementando agentes de aprendizaje por refuerzo para tres ambientes de robótica diferentes:\n",
        "\n",
        "1. **AdroitHandDoor-v1**: Manipulación de puerta con mano robótica\n",
        "2. **FetchReachDense-v4**: Alcanzar objetivo con brazo robótico\n",
        "3. **HandReachDense-v3**: Coordinación de dedos con mano antropomórfica\n",
        "\n",
        "## Tabla de Contenidos\n",
        "1. [Reporte Profesional](#reporte)\n",
        "2. [Instalación de Dependencias](#instalacion)\n",
        "3. [Configuración Base](#configuracion)\n",
        "4. [AdroitHandDoor-v1 - Apertura de Puerta](#adroit)\n",
        "5. [FetchReachDense-v4 - Alcanzar Objetivo](#fetch)\n",
        "6. [HandReachDense-v3 - Coordinación de Dedos](#shadow)\n",
        "7. [Análisis Comparativo](#analisis)\n",
        "8. [Conclusiones](#conclusiones)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrJUiClTuaHO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laCaMQD2Gljj"
      },
      "source": [
        "# 1. Reporte Profesional\n",
        "\n",
        "## 1.1 Selección y Justificación de Algoritmos\n",
        "\n",
        "Para este reto se seleccionaron los siguientes algoritmos basados en las características específicas de cada ambiente:\n",
        "\n",
        "### AdroitHandDoor-v1:\n",
        "- **Algoritmo Principal**: **SAC (Soft Actor-Critic)**\n",
        "- **Algoritmo Secundario**: **PPO (Proximal Policy Optimization)**\n",
        "\n",
        "**Justificación**: La tarea de abrir una puerta requiere exploración eficiente y control preciso. SAC es ideal porque:\n",
        "- Maximiza tanto recompensa como entropía, favoreciendo la exploración\n",
        "- Es off-policy, permitiendo mejor eficiencia de muestreo\n",
        "- Maneja bien espacios de acción continuos de alta dimensionalidad (30 grados de libertad)\n",
        "- Su robustez lo hace ideal para tareas de manipulación compleja\n",
        "\n",
        "### FetchReachDense-v4:\n",
        "- **Algoritmo Principal**: **TD3 (Twin Delayed Deep Deterministic Policy Gradient)**\n",
        "- **Algoritmo Secundario**: **DDPG (Deep Deterministic Policy Gradient)**\n",
        "\n",
        "**Justificación**: La tarea de alcanzar un objetivo requiere control determinístico preciso. TD3 es superior porque:\n",
        "- Diseñado específicamente para control continuo\n",
        "- Reduce el sesgo de maximización mediante twin critics\n",
        "- Las actualizaciones retardadas mejoran la estabilidad\n",
        "- Excelente para tareas de posicionamiento preciso\n",
        "\n",
        "### HandReachDense-v3:\n",
        "- **Algoritmo Principal**: **PPO (Proximal Policy Optimization)**\n",
        "- **Algoritmo Secundario**: **SAC (Soft Actor-Critic)**\n",
        "\n",
        "**Justificación**: La coordinación de dedos requiere políticas estables y confiables. PPO es ideal porque:\n",
        "- Garantiza actualizaciones de política estables mediante clipping\n",
        "- Maneja bien la coordinación de múltiples articulaciones\n",
        "- Balance óptimo entre exploración y explotación\n",
        "- Menos propenso a divergencias en espacios de acción complejos\n",
        "\n",
        "## 1.2 Condiciones de Ejecución\n",
        "\n",
        "### Parámetros de Aprendizaje:\n",
        "- **Pasos de entrenamiento**: 500,000 - 1,000,000 según la complejidad\n",
        "- **Learning rate**: 3e-4 (adaptativo según algoritmo)\n",
        "- **Batch size**: 256\n",
        "- **Buffer size**: 1,000,000\n",
        "- **Gamma (factor de descuento)**: 0.99\n",
        "\n",
        "### Librerías Utilizadas:\n",
        "- `gymnasium-robotics`: Ambientes de robótica\n",
        "- `stable-baselines3`: Implementaciones de algoritmos RL\n",
        "- `sb3-contrib`: Algoritmos adicionales (TRPO)\n",
        "- `torch`: Backend de deep learning\n",
        "- `mujoco`: Simulador físico\n",
        "\n",
        "### Hardware de Ejecución:\n",
        "- **Plataforma**: Google Colab Pro\n",
        "- **GPU**: A100\n",
        "- **RAM**:\n",
        "- **Tiempo estimado por ambiente**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMrLxw2NGljk"
      },
      "source": [
        "# 2. Instalación de Dependencias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_Axv8MuGljl",
        "outputId": "c0a61699-08c6-4029-f639-b4434063c710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.2/26.2 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m852.5/852.5 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h Todas las dependencias han sido instaladas correctamente.\n"
          ]
        }
      ],
      "source": [
        "# Instalación de dependencias del sistema\n",
        "!apt update -y > /dev/null\n",
        "!apt install -y xvfb ffmpeg > /dev/null\n",
        "\n",
        "# Instalación de paquetes de Python especializados en robótica\n",
        "!pip install gymnasium-robotics -q\n",
        "!pip install pyvirtualdisplay imageio[ffmpeg] -q\n",
        "!pip install stable-baselines3[extra] sb3-contrib -q\n",
        "!pip install tensorboard -q\n",
        "\n",
        "print(\" Todas las dependencias han sido instaladas correctamente.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND_b5NpbtfBP",
        "outputId": "67434961-00e5-49f9-ead7-b71375ea3215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎛️ CONFIGURACIÓN DE ENTRENAMIENTO: QUICK\n",
            "============================================================\n",
            "\n",
            "📋 AdroitHandDoor-v1:\n",
            "   Timesteps: 200,000\n",
            "   Tiempo estimado: ~20.0 min\n",
            "   🚀 Entrenamiento rápido - Resultados básicos\n",
            "\n",
            "📋 FetchReachDense-v4:\n",
            "   Timesteps: 100,000\n",
            "   Tiempo estimado: ~10.0 min\n",
            "   🚀 Entrenamiento rápido - Resultados básicos\n",
            "\n",
            "📋 HandReachDense-v3:\n",
            "   Timesteps: 150,000\n",
            "   Tiempo estimado: ~15.0 min\n",
            "   🚀 Entrenamiento rápido - Resultados básicos\n",
            "\n",
            "⏱️ TIEMPO TOTAL ESTIMADO: ~45.0 minutos\n",
            "🎯 MODO SELECCIONADO: QUICK\n",
            "\n",
            "💡 Ideal para pruebas rápidas y demostraciones\n",
            "\n",
            "✅ Configuración cargada exitosamente en modo: QUICK\n",
            "📝 Para cambiar el modo, modifica la variable TRAINING_MODE y ejecuta esta celda nuevamente\n"
          ]
        }
      ],
      "source": [
        "class TrainingConfig:\n",
        "    \"\"\"Configuración centralizada para todos los entrenamientos\"\"\"\n",
        "\n",
        "    def __init__(self, mode=\"balanced\"):\n",
        "        \"\"\"\n",
        "        Modos disponibles:\n",
        "        - \"quick\": Entrenamiento rápido (5-15 min por ambiente)\n",
        "        - \"balanced\": Balance tiempo/calidad (15-45 min por ambiente)\n",
        "        - \"quality\": Máxima calidad (45-120 min por ambiente)\n",
        "        - \"research\": Entrenamiento extensivo (2-4 horas por ambiente)\n",
        "        \"\"\"\n",
        "        self.mode = mode\n",
        "        self.configs = self._get_configs()\n",
        "\n",
        "    def _get_configs(self):\n",
        "        \"\"\"Configuraciones predefinidas por modo\"\"\"\n",
        "        configs = {\n",
        "            \"quick\": {\n",
        "                \"AdroitHandDoor-v1\": {\n",
        "                    \"total_timesteps\": 200_000,\n",
        "                    \"eval_freq\": 5_000,\n",
        "                    \"n_eval_episodes\": 3,\n",
        "                    \"patience_episodes\": 20,\n",
        "                    \"batch_size\": 256,\n",
        "                    \"buffer_size\": 100_000,\n",
        "                    \"description\": \"🚀 Entrenamiento rápido - Resultados básicos\"\n",
        "                },\n",
        "                \"FetchReachDense-v4\": {\n",
        "                    \"total_timesteps\": 100_000,\n",
        "                    \"eval_freq\": 2_500,\n",
        "                    \"n_eval_episodes\": 3,\n",
        "                    \"patience_episodes\": 15,\n",
        "                    \"batch_size\": 256,\n",
        "                    \"buffer_size\": 50_000,\n",
        "                    \"description\": \"🚀 Entrenamiento rápido - Resultados básicos\"\n",
        "                },\n",
        "                \"HandReachDense-v3\": {\n",
        "                    \"total_timesteps\": 150_000,\n",
        "                    \"eval_freq\": 3_750,\n",
        "                    \"n_eval_episodes\": 3,\n",
        "                    \"patience_episodes\": 20,\n",
        "                    \"batch_size\": 256,\n",
        "                    \"buffer_size\": 75_000,\n",
        "                    \"description\": \"🚀 Entrenamiento rápido - Resultados básicos\"\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"balanced\": {\n",
        "                \"AdroitHandDoor-v1\": {\n",
        "                    \"total_timesteps\": 800_000,\n",
        "                    \"eval_freq\": 20_000,\n",
        "                    \"n_eval_episodes\": 5,\n",
        "                    \"patience_episodes\": 50,\n",
        "                    \"batch_size\": 512,\n",
        "                    \"buffer_size\": 1_000_000,\n",
        "                    \"description\": \"⚖️ Balance tiempo/calidad - Resultados buenos\"\n",
        "                },\n",
        "                \"FetchReachDense-v4\": {\n",
        "                    \"total_timesteps\": 300_000,\n",
        "                    \"eval_freq\": 10_000,\n",
        "                    \"n_eval_episodes\": 5,\n",
        "                    \"patience_episodes\": 30,\n",
        "                    \"batch_size\": 512,\n",
        "                    \"buffer_size\": 500_000,\n",
        "                    \"description\": \"⚖️ Balance tiempo/calidad - Resultados buenos\"\n",
        "                },\n",
        "                \"HandReachDense-v3\": {\n",
        "                    \"total_timesteps\": 600_000,\n",
        "                    \"eval_freq\": 15_000,\n",
        "                    \"n_eval_episodes\": 5,\n",
        "                    \"patience_episodes\": 40,\n",
        "                    \"batch_size\": 512,\n",
        "                    \"buffer_size\": 800_000,\n",
        "                    \"description\": \"⚖️ Balance tiempo/calidad - Resultados buenos\"\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"quality\": {\n",
        "                \"AdroitHandDoor-v1\": {\n",
        "                    \"total_timesteps\": 1_500_000,\n",
        "                    \"eval_freq\": 30_000,\n",
        "                    \"n_eval_episodes\": 10,\n",
        "                    \"patience_episodes\": 100,\n",
        "                    \"batch_size\": 1024,\n",
        "                    \"buffer_size\": 2_000_000,\n",
        "                    \"description\": \"🏆 Alta calidad - Resultados excelentes\"\n",
        "                },\n",
        "                \"FetchReachDense-v4\": {\n",
        "                    \"total_timesteps\": 800_000,\n",
        "                    \"eval_freq\": 20_000,\n",
        "                    \"n_eval_episodes\": 10,\n",
        "                    \"patience_episodes\": 75,\n",
        "                    \"batch_size\": 1024,\n",
        "                    \"buffer_size\": 1_500_000,\n",
        "                    \"description\": \"🏆 Alta calidad - Resultados excelentes\"\n",
        "                },\n",
        "                \"HandReachDense-v3\": {\n",
        "                    \"total_timesteps\": 1_200_000,\n",
        "                    \"eval_freq\": 30_000,\n",
        "                    \"n_eval_episodes\": 10,\n",
        "                    \"patience_episodes\": 80,\n",
        "                    \"batch_size\": 1024,\n",
        "                    \"buffer_size\": 1_800_000,\n",
        "                    \"description\": \"🏆 Alta calidad - Resultados excelentes\"\n",
        "                }\n",
        "            },\n",
        "\n",
        "            \"research\": {\n",
        "                \"AdroitHandDoor-v1\": {\n",
        "                    \"total_timesteps\": 3_000_000,\n",
        "                    \"eval_freq\": 50_000,\n",
        "                    \"n_eval_episodes\": 20,\n",
        "                    \"patience_episodes\": 200,\n",
        "                    \"batch_size\": 1024,\n",
        "                    \"buffer_size\": 3_000_000,\n",
        "                    \"description\": \"🔬 Investigación - Máxima calidad posible\"\n",
        "                },\n",
        "                \"FetchReachDense-v4\": {\n",
        "                    \"total_timesteps\": 1_500_000,\n",
        "                    \"eval_freq\": 30_000,\n",
        "                    \"n_eval_episodes\": 20,\n",
        "                    \"patience_episodes\": 150,\n",
        "                    \"batch_size\": 1024,\n",
        "                    \"buffer_size\": 2_500_000,\n",
        "                    \"description\": \"🔬 Investigación - Máxima calidad posible\"\n",
        "                },\n",
        "                \"HandReachDense-v3\": {\n",
        "                    \"total_timesteps\": 2_500_000,\n",
        "                    \"eval_freq\": 50_000,\n",
        "                    \"n_eval_episodes\": 20,\n",
        "                    \"patience_episodes\": 180,\n",
        "                    \"batch_size\": 1024,\n",
        "                    \"buffer_size\": 3_000_000,\n",
        "                    \"description\": \"🔬 Investigación - Máxima calidad posible\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        return configs[self.mode]\n",
        "\n",
        "    def get_config(self, env_name):\n",
        "        \"\"\"Obtiene configuración para un ambiente específico\"\"\"\n",
        "        return self.configs.get(env_name, {})\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Muestra resumen de la configuración actual\"\"\"\n",
        "        print(f\"🎛️ CONFIGURACIÓN DE ENTRENAMIENTO: {self.mode.upper()}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        total_time_estimate = 0\n",
        "        for env_name, config in self.configs.items():\n",
        "            # Estimación de tiempo basada en timesteps\n",
        "            time_estimate = config[\"total_timesteps\"] / 10000  # Aproximación\n",
        "            total_time_estimate += time_estimate\n",
        "\n",
        "            print(f\"\\n📋 {env_name}:\")\n",
        "            print(f\"   Timesteps: {config['total_timesteps']:,}\")\n",
        "            print(f\"   Tiempo estimado: ~{time_estimate:.1f} min\")\n",
        "            print(f\"   {config['description']}\")\n",
        "\n",
        "        print(f\"\\n⏱️ TIEMPO TOTAL ESTIMADO: ~{total_time_estimate:.1f} minutos\")\n",
        "        print(f\"🎯 MODO SELECCIONADO: {self.mode.upper()}\")\n",
        "\n",
        "        # Recomendaciones por modo\n",
        "        recommendations = {\n",
        "            \"quick\": \"💡 Ideal para pruebas rápidas y demostraciones\",\n",
        "            \"balanced\": \"💡 Recomendado para la mayoría de casos de uso\",\n",
        "            \"quality\": \"💡 Para resultados de alta calidad en producción\",\n",
        "            \"research\": \"💡 Para investigación y publicaciones científicas\"\n",
        "        }\n",
        "        print(f\"\\n{recommendations.get(self.mode, '')}\")\n",
        "\n",
        "# ================================================\n",
        "# 🔧 CONFIGURACIÓN PERSONALIZADA\n",
        "# ================================================\n",
        "# Cambia este valor para ajustar el modo de entrenamiento\n",
        "TRAINING_MODE = \"quick\"  # Opciones: \"quick\", \"balanced\", \"quality\", \"research\"\n",
        "\n",
        "# Crear instancia de configuración\n",
        "config = TrainingConfig(mode=TRAINING_MODE)\n",
        "\n",
        "# Mostrar resumen\n",
        "config.print_summary()\n",
        "\n",
        "# ================================================\n",
        "# 🎯 CONFIGURACIONES ESPECÍFICAS POR ALGORITMO\n",
        "# ================================================\n",
        "\n",
        "# Configuraciones adicionales que se aplicarán según el algoritmo\n",
        "ALGORITHM_CONFIGS = {\n",
        "    \"SAC\": {\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"tau\": 0.005,\n",
        "        \"gamma\": 0.99,\n",
        "        \"train_freq\": 4,\n",
        "        \"gradient_steps\": 4,\n",
        "        \"ent_coef\": 0.1,\n",
        "        \"target_update_interval\": 1\n",
        "    },\n",
        "    \"PPO\": {\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"n_epochs\": 10,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"clip_range\": 0.2,\n",
        "        \"ent_coef\": 0.01,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 0.5\n",
        "    },\n",
        "    \"TD3\": {\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"tau\": 0.005,\n",
        "        \"gamma\": 0.99,\n",
        "        \"train_freq\": 4,\n",
        "        \"gradient_steps\": 4,\n",
        "        \"policy_delay\": 2,\n",
        "        \"target_policy_noise\": 0.2,\n",
        "        \"target_noise_clip\": 0.5\n",
        "    },\n",
        "    \"DDPG\": {\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"tau\": 0.005,\n",
        "        \"gamma\": 0.99,\n",
        "        \"train_freq\": 4,\n",
        "        \"gradient_steps\": 4\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"\\n✅ Configuración cargada exitosamente en modo: {TRAINING_MODE.upper()}\")\n",
        "print(\"📝 Para cambiar el modo, modifica la variable TRAINING_MODE y ejecuta esta celda nuevamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_koBNo3Gljm"
      },
      "source": [
        "# 3. Configuración Base\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScItCqc0Gljm",
        "outputId": "08ee1494-a804-4e45-97d2-9bc691a3dbe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Configuración completada.\n"
          ]
        }
      ],
      "source": [
        "# Importaciones necesarias\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "import gymnasium_robotics\n",
        "import imageio\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import HTML, clear_output\n",
        "from base64 import b64encode\n",
        "\n",
        "# Stable Baselines3\n",
        "from stable_baselines3 import SAC, PPO, DDPG, TD3\n",
        "from sb3_contrib import TRPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Configuración de display virtual\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# Verificar GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"  Dispositivo de entrenamiento: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\" GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "clear_output()\n",
        "print(\" Configuración completada.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fuHU_TWGljn",
        "outputId": "24d64bcb-3a36-4d08-9282-0645683a78b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Clases de entrenamiento configuradas.\n"
          ]
        }
      ],
      "source": [
        "# Clase base para entrenamiento de agentes robóticos\n",
        "class RoboticsTrainer:\n",
        "    def __init__(self, env_name, algorithm_fn, training_config=None, log_dir=\"./logs\"):\n",
        "        self.env_name = env_name\n",
        "        self.log_dir = log_dir\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        # Usar configuración centralizada si se proporciona\n",
        "        if training_config is None:\n",
        "            training_config = config  # Usar la configuración global\n",
        "\n",
        "        self.training_config = training_config.get_config(env_name)\n",
        "        self.total_timesteps = self.training_config.get(\"total_timesteps\", 500_000)\n",
        "\n",
        "        # Crear ambiente\n",
        "        self.env = make_vec_env(env_name, n_envs=1)\n",
        "\n",
        "        # Crear modelo con configuración optimizada\n",
        "        self.model = algorithm_fn(self.env, self.training_config)\n",
        "\n",
        "        # Umbrales de éxito específicos por ambiente\n",
        "        thresholds = {\n",
        "            \"AdroitHandDoor-v1\": 2000,      # Apertura de puerta\n",
        "            \"FetchReachDense-v4\": -5,       # Alcanzar objetivo (negativo típico)\n",
        "            \"HandReachDense-v3\": -2         # Coordinación dedos (negativo típico)\n",
        "        }\n",
        "\n",
        "        # Callback con parada temprana optimizada por ambiente\n",
        "        threshold = thresholds.get(env_name, 0)\n",
        "        patience = self.training_config.get(\"patience_episodes\", 50)\n",
        "        self.callback = RewardLoggerCallback(\n",
        "            success_threshold=threshold,\n",
        "            patience_episodes=patience\n",
        "        )\n",
        "\n",
        "    def train(self, save_path):\n",
        "        print(f\" Iniciando entrenamiento de {self.model.__class__.__name__} en {self.env_name}\")\n",
        "        print(f\" Pasos totales: {self.total_timesteps:,}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        self.model.learn(\n",
        "            total_timesteps=self.total_timesteps,\n",
        "            callback=self.callback,\n",
        "            log_interval=10\n",
        "        )\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        self.model.save(save_path)\n",
        "        print(f\" Entrenamiento completado en {elapsed/60:.1f} minutos\")\n",
        "        print(f\" Modelo guardado en: {save_path}\")\n",
        "\n",
        "        return elapsed\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        if not self.callback.episode_rewards:\n",
        "            print(\" No hay datos de entrenamiento para mostrar\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Gráfica de recompensas\n",
        "        plt.subplot(1, 2, 1)\n",
        "        rewards = self.callback.episode_rewards\n",
        "        plt.plot(rewards, alpha=0.6, label=\"Recompensa por episodio\")\n",
        "\n",
        "        # Media móvil\n",
        "        window = min(100, len(rewards) // 10)\n",
        "        if window > 1:\n",
        "            moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "            plt.plot(range(window-1, len(rewards)), moving_avg,\n",
        "                    color='red', linewidth=2, label=f\"Media móvil ({window})\")\n",
        "\n",
        "        plt.xlabel(\"Episodios\")\n",
        "        plt.ylabel(\"Recompensa\")\n",
        "        plt.title(f\"Progreso de Entrenamiento - {self.env_name}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Estadísticas\n",
        "        plt.subplot(1, 2, 2)\n",
        "        final_rewards = rewards[-50:] if len(rewards) >= 50 else rewards\n",
        "        plt.hist(final_rewards, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        plt.axvline(np.mean(final_rewards), color='red', linestyle='--',\n",
        "                   label=f'Media: {np.mean(final_rewards):.2f}')\n",
        "        plt.xlabel(\"Recompensa\")\n",
        "        plt.ylabel(\"Frecuencia\")\n",
        "        plt.title(\"Distribución de Recompensas Finales\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate_agent(self, model_path, n_episodes=10):\n",
        "        \"\"\"Evalúa el agente entrenado\"\"\"\n",
        "        model = self.model.__class__.load(model_path)\n",
        "        env = gym.make(self.env_name)\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "\n",
        "        for episode in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                episode_reward += reward\n",
        "                episode_length += 1\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        print(f\" Evaluación del agente ({n_episodes} episodios):\")\n",
        "        print(f\"   Recompensa promedio: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
        "        print(f\"   Longitud promedio: {np.mean(episode_lengths):.1f} ± {np.std(episode_lengths):.1f}\")\n",
        "        print(f\"   Mejor recompensa: {np.max(episode_rewards):.2f}\")\n",
        "\n",
        "        return episode_rewards, episode_lengths\n",
        "\n",
        "    def create_demo_video(self, model_path, video_name, n_episodes=3):\n",
        "        \"\"\"Crea video demostrativo del agente\"\"\"\n",
        "        model = self.model.__class__.load(model_path)\n",
        "        env = gym.make(self.env_name, render_mode=\"rgb_array\")\n",
        "\n",
        "        all_frames = []\n",
        "        total_reward = 0\n",
        "\n",
        "        for episode in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "\n",
        "            # Agregar frame de título\n",
        "            frame = env.render()\n",
        "            all_frames.extend([frame] * 30)  # 1 segundo a 30 FPS\n",
        "\n",
        "            while not done:\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                episode_reward += reward\n",
        "\n",
        "                frame = env.render()\n",
        "                all_frames.append(frame)\n",
        "\n",
        "            total_reward += episode_reward\n",
        "            print(f\"Episodio {episode + 1}: Recompensa = {episode_reward:.2f}\")\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        # Guardar video\n",
        "        imageio.mimsave(video_name, all_frames, fps=30)\n",
        "        print(f\" Video guardado: {video_name}\")\n",
        "        print(f\" Recompensa total promedio: {total_reward/n_episodes:.2f}\")\n",
        "\n",
        "        # Mostrar video en notebook\n",
        "        mp4 = open(video_name, \"rb\").read()\n",
        "        data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "        return HTML(f\"\"\"\n",
        "        <video width=800 controls>\n",
        "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "        \"\"\")\n",
        "\n",
        "# Callback personalizado con PARADA TEMPRANA\n",
        "class RewardLoggerCallback(BaseCallback):\n",
        "    def __init__(self, success_threshold=2000, patience_episodes=200):\n",
        "        super().__init__()\n",
        "        self.episode_rewards = []\n",
        "        self.episode_reward = 0\n",
        "        self.episode_count = 0\n",
        "        self.success_threshold = success_threshold\n",
        "        self.patience_episodes = patience_episodes\n",
        "        self.episodes_above_threshold = 0\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if \"rewards\" in self.locals:\n",
        "            self.episode_reward += self.locals[\"rewards\"][0]\n",
        "\n",
        "        if \"dones\" in self.locals and self.locals[\"dones\"][0]:\n",
        "            self.episode_rewards.append(self.episode_reward)\n",
        "            self.episode_count += 1\n",
        "\n",
        "            # Verificar criterio de éxito\n",
        "            if self.episode_reward >= self.success_threshold:\n",
        "                self.episodes_above_threshold += 1\n",
        "            else:\n",
        "                self.episodes_above_threshold = 0\n",
        "\n",
        "            # Log cada 50 episodios\n",
        "            if self.episode_count % 50 == 0:\n",
        "                recent_rewards = self.episode_rewards[-50:]\n",
        "                avg_reward = np.mean(recent_rewards)\n",
        "                print(f\"Episodio {self.episode_count}: Recompensa promedio (últimos 50): {avg_reward:.2f}\")\n",
        "\n",
        "                # PARADA TEMPRANA: Si mantiene buen rendimiento\n",
        "                if avg_reward >= self.success_threshold and len(recent_rewards) >= 50:\n",
        "                    print(f\" ¡ÉXITO SOSTENIDO! Promedio {avg_reward:.0f} >= {self.success_threshold}\")\n",
        "                    print(f\" Deteniendo entrenamiento temprano - ¡Objetivo alcanzado!\")\n",
        "                    return False  # Detener entrenamiento\n",
        "\n",
        "            # Parada temprana por episodios consecutivos exitosos\n",
        "            if self.episodes_above_threshold >= self.patience_episodes:\n",
        "                print(f\" ¡DOMINIO COMPLETO! {self.episodes_above_threshold} episodios consecutivos >= {self.success_threshold}\")\n",
        "                print(f\" Deteniendo entrenamiento - ¡Agente ha dominado la tarea!\")\n",
        "                return False  # Detener entrenamiento\n",
        "\n",
        "            self.episode_reward = 0\n",
        "\n",
        "        return True\n",
        "\n",
        "print(\" Clases de entrenamiento configuradas.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbQ8xpQ2Gljo"
      },
      "source": [
        "# 4. AdroitHandDoor-v1 - Apertura de Puerta\n",
        "\n",
        "## 4.1 Descripción del Ambiente\n",
        "\n",
        "El ambiente **AdroitHandDoor-v1** simula una mano robótica de Shadow Robotics que debe aprender a abrir una puerta. La mano tiene 30 grados de libertad y debe:\n",
        "\n",
        "- Tomar la manija de la puerta\n",
        "- Realizar los movimientos necesarios para abrirla\n",
        "- Mantener el control durante todo el proceso\n",
        "\n",
        "**Características:**\n",
        "- **Espacio de observación**: 39 dimensiones (posiciones, velocidades, información de la puerta)\n",
        "- **Espacio de acción**: 28 dimensiones (control de articulaciones)\n",
        "- **Recompensa**: Densa, basada en proximidad a la manija y progreso de apertura\n",
        "\n",
        "## 4.2 Implementación con SAC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9gwrlEQGljo",
        "outputId": "91b203c1-b1c3-46cd-ecea-5215e2fb91d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Iniciando entrenamiento SAC para AdroitHandDoor-v1\n",
            "Using cuda device\n",
            " Iniciando entrenamiento de SAC en AdroitHandDoor-v1\n",
            " Pasos totales: 200,000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -45.7    |\n",
            "| time/              |          |\n",
            "|    episodes        | 10       |\n",
            "|    fps             | 1159     |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -45.8    |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 1171     |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -45.8    |\n",
            "| time/              |          |\n",
            "|    episodes        | 30       |\n",
            "|    fps             | 1177     |\n",
            "|    time_elapsed    | 5        |\n",
            "|    total_timesteps | 6000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -45.9    |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 1182     |\n",
            "|    time_elapsed    | 6        |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "Episodio 50: Recompensa promedio (últimos 50): -45.86\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -45.9    |\n",
            "| time/              |          |\n",
            "|    episodes        | 50       |\n",
            "|    fps             | 1185     |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -45.9    |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 379      |\n",
            "|    time_elapsed    | 31       |\n",
            "|    total_timesteps | 12000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -17      |\n",
            "|    critic_loss     | 0.0105   |\n",
            "|    ent_coef        | 0.1      |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1996     |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Configuración SAC para AdroitHandDoor\n",
        "def create_sac_adroit(env, training_config):\n",
        "    # Combinar configuración centralizada con parámetros específicos del algoritmo\n",
        "    sac_config = ALGORITHM_CONFIGS[\"SAC\"].copy()\n",
        "    sac_config.update({\n",
        "        \"buffer_size\": training_config.get(\"buffer_size\", 1_200_000),\n",
        "        \"batch_size\": training_config.get(\"batch_size\", 512),\n",
        "        \"learning_starts\": 10_000,\n",
        "        \"verbose\": 1,\n",
        "        \"device\": device\n",
        "    })\n",
        "\n",
        "    model = SAC(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        **sac_config\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Entrenar agente SAC para AdroitHandDoor\n",
        "print(\" Iniciando entrenamiento SAC para AdroitHandDoor-v1\")\n",
        "adroit_trainer = RoboticsTrainer(\n",
        "    env_name=\"AdroitHandDoor-v1\",\n",
        "    algorithm_fn=create_sac_adroit,\n",
        "    training_config=config,\n",
        "    log_dir=\"./logs/adroit_sac\"\n",
        ")\n",
        "\n",
        "# Entrenamiento\n",
        "training_time = adroit_trainer.train(\"adroit_sac_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aes6y8WqGljp"
      },
      "outputs": [],
      "source": [
        "# Visualizar progreso de entrenamiento\n",
        "adroit_trainer.plot_training_progress()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFPbmucRHhJu"
      },
      "outputs": [],
      "source": [
        "# Evaluación del agente\n",
        "adroit_rewards, adroit_lengths = adroit_trainer.evaluate_agent(\"adroit_sac_model\", n_episodes=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZsSlFMKHg_G"
      },
      "outputs": [],
      "source": [
        "# Crear video demostrativo\n",
        "adroit_video = adroit_trainer.create_demo_video(\n",
        "    \"adroit_sac_model\",\n",
        "    \"adroit_door_sac_demo.mp4\",\n",
        "    n_episodes=2\n",
        ")\n",
        "adroit_video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jocHfQWOGljq"
      },
      "source": [
        "## 4.3 Implementación Alternativa con PPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWZZnbDVGljq"
      },
      "outputs": [],
      "source": [
        "# Configuración PPO\n",
        "def create_ppo_adroit():\n",
        "    env = make_vec_env(\"AdroitHandDoor-v1\", n_envs=8)\n",
        "\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        learning_rate=3e-4,\n",
        "        n_steps=4096,\n",
        "        batch_size=512,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        clip_range_vf=None,\n",
        "        ent_coef=0.01,\n",
        "        vf_coef=0.5,\n",
        "        max_grad_norm=0.5,\n",
        "        verbose=1,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Entrenar PPO para comparación\n",
        "print(\" Iniciando entrenamiento PPO para AdroitHandDoor-v1 (comparación)\")\n",
        "adroit_ppo_trainer = RoboticsTrainer(\n",
        "    env_name=\"AdroitHandDoor-v1\",\n",
        "    algorithm_fn=lambda env: create_ppo_adroit(),\n",
        "    total_timesteps=1_000_000,\n",
        "    log_dir=\"./logs/adroit_ppo\"\n",
        ")\n",
        "\n",
        "training_time_ppo = adroit_ppo_trainer.train(\"adroit_ppo_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIixIFGsGljq"
      },
      "outputs": [],
      "source": [
        "# Comparar resultados\n",
        "adroit_ppo_trainer.plot_training_progress()\n",
        "adroit_ppo_rewards, _ = adroit_ppo_trainer.evaluate_agent(\"adroit_ppo_model\", n_episodes=20)\n",
        "\n",
        "print(\"\\n Comparación AdroitHandDoor:\")\n",
        "print(f\"SAC - Recompensa promedio: {np.mean(adroit_rewards):.2f} ± {np.std(adroit_rewards):.2f}\")\n",
        "print(f\"PPO - Recompensa promedio: {np.mean(adroit_ppo_rewards):.2f} ± {np.std(adroit_ppo_rewards):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO5hgh96Gljq"
      },
      "source": [
        "# 5. FetchReachDense-v4 - Alcanzar Objetivo\n",
        "\n",
        "## 5.1 Descripción del Ambiente\n",
        "\n",
        "El ambiente **FetchReachDense-v4** presenta un brazo robótico Fetch que debe alcanzar un objetivo específico en el espacio 3D. El robot tiene:\n",
        "\n",
        "- 7 grados de libertad en el brazo\n",
        "- Una pinza de dos dedos\n",
        "- Control mediante desplazamientos cartesianos\n",
        "\n",
        "**Características:**\n",
        "- **Espacio de observación**: Incluye posiciones del brazo, objetivo y estado de la pinza\n",
        "- **Espacio de acción**: 4 dimensiones (3D + apertura de pinza)\n",
        "- **Recompensa**: Densa, basada en distancia al objetivo\n",
        "\n",
        "## 5.2 Implementación con TD3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx6IEI8zGljr"
      },
      "outputs": [],
      "source": [
        "# Configuración TD3 para FetchReachDense\n",
        "def create_td3_fetch(env, training_config):\n",
        "    # Combinar configuración centralizada con parámetros específicos del algoritmo\n",
        "    td3_config = ALGORITHM_CONFIGS[\"TD3\"].copy()\n",
        "    td3_config.update({\n",
        "        \"buffer_size\": training_config.get(\"buffer_size\", 500_000),\n",
        "        \"batch_size\": training_config.get(\"batch_size\", 512),\n",
        "        \"learning_starts\": 5_000,\n",
        "        \"verbose\": 1,\n",
        "        \"device\": device\n",
        "    })\n",
        "\n",
        "    # Agregar ruido para exploración\n",
        "    n_actions = env.action_space.shape[-1]\n",
        "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "    td3_config[\"action_noise\"] = action_noise\n",
        "\n",
        "    model = TD3(\n",
        "        \"MultiInputPolicy\",\n",
        "        env,\n",
        "        **td3_config\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Entrenar agente TD3 para FetchReachDense\n",
        "print(\" Iniciando entrenamiento TD3 para FetchReachDense-v4\")\n",
        "fetch_trainer = RoboticsTrainer(\n",
        "    env_name=\"FetchReachDense-v4\",\n",
        "    algorithm_fn=create_td3_fetch,\n",
        "    training_config=config,\n",
        "    log_dir=\"./logs/fetch_td3\"\n",
        ")\n",
        "\n",
        "# Entrenamiento\n",
        "fetch_training_time = fetch_trainer.train(\"fetch_td3_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW_ZoWDTGljr"
      },
      "outputs": [],
      "source": [
        "# Visualizar progreso de entrenamiento\n",
        "fetch_trainer.plot_training_progress()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvGFvHnIGljs"
      },
      "outputs": [],
      "source": [
        "# Evaluación del agente\n",
        "fetch_rewards, fetch_lengths = fetch_trainer.evaluate_agent(\"fetch_td3_model\", n_episodes=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dh7ibVIGljs"
      },
      "outputs": [],
      "source": [
        "# Crear video demostrativo\n",
        "fetch_video = fetch_trainer.create_demo_video(\n",
        "    \"fetch_td3_model\",\n",
        "    \"fetch_reach_td3_demo.mp4\",\n",
        "    n_episodes=3\n",
        ")\n",
        "fetch_video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyLG1wDKGljs"
      },
      "source": [
        "## 5.3 Implementación Alternativa con DDPG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoSWlW1IGljs"
      },
      "outputs": [],
      "source": [
        "# Configuración DDPG para comparación\n",
        "def create_ddpg_fetch(env, training_config):\n",
        "    # Combinar configuración centralizada con parámetros específicos del algoritmo\n",
        "    ddpg_config = ALGORITHM_CONFIGS[\"DDPG\"].copy()\n",
        "    ddpg_config.update({\n",
        "        \"buffer_size\": training_config.get(\"buffer_size\", 500_000),\n",
        "        \"batch_size\": training_config.get(\"batch_size\", 512),\n",
        "        \"learning_starts\": 5_000,\n",
        "        \"verbose\": 1,\n",
        "        \"device\": device\n",
        "    })\n",
        "\n",
        "    # Agregar ruido para exploración\n",
        "    n_actions = env.action_space.shape[-1]\n",
        "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "    ddpg_config[\"action_noise\"] = action_noise\n",
        "\n",
        "    model = DDPG(\n",
        "        \"MultiInputPolicy\",\n",
        "        env,\n",
        "        **ddpg_config\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Entrenar DDPG para comparación\n",
        "print(\" Iniciando entrenamiento DDPG para FetchReachDense-v4 (comparación)\")\n",
        "fetch_ddpg_trainer = RoboticsTrainer(\n",
        "    env_name=\"FetchReachDense-v4\",\n",
        "    algorithm_fn=create_ddpg_fetch,\n",
        "    training_config=config,\n",
        "    log_dir=\"./logs/fetch_ddpg\"\n",
        ")\n",
        "\n",
        "fetch_ddpg_training_time = fetch_ddpg_trainer.train(\"fetch_ddpg_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbbp2Z42Gljs"
      },
      "outputs": [],
      "source": [
        "# Comparar resultados\n",
        "fetch_ddpg_trainer.plot_training_progress()\n",
        "fetch_ddpg_rewards, _ = fetch_ddpg_trainer.evaluate_agent(\"fetch_ddpg_model\", n_episodes=20)\n",
        "\n",
        "print(\"\\n Comparación FetchReachDense:\")\n",
        "print(f\"TD3  - Recompensa promedio: {np.mean(fetch_rewards):.2f} ± {np.std(fetch_rewards):.2f}\")\n",
        "print(f\"DDPG - Recompensa promedio: {np.mean(fetch_ddpg_rewards):.2f} ± {np.std(fetch_ddpg_rewards):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWFj2JkXGljt"
      },
      "source": [
        "# 6. HandReachDense-v3 - Coordinación de Dedos\n",
        "\n",
        "## 6.1 Descripción del Ambiente\n",
        "\n",
        "El ambiente **HandReachDense-v3** presenta una mano antropomórfica Shadow Dexterous Hand con 20 grados de libertad. La tarea consiste en:\n",
        "\n",
        "- Tocar la punta de un dedo específico con el pulgar\n",
        "- El dedo objetivo se selecciona aleatoriamente (índice, medio, anular, meñique)\n",
        "- Requiere coordinación precisa de múltiples articulaciones\n",
        "\n",
        "**Características:**\n",
        "- **Espacio de observación**: Posiciones y velocidades de todas las articulaciones\n",
        "- **Espacio de acción**: 20 dimensiones (control de articulaciones)\n",
        "- **Recompensa**: Densa, basada en distancia entre dedos\n",
        "\n",
        "## 6.2 Implementación con PPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5251n4hGljt"
      },
      "outputs": [],
      "source": [
        "# Configuración PPO para HandReachDense\n",
        "def create_ppo_shadow(env, training_config):\n",
        "    # Combinar configuración centralizada con parámetros específicos del algoritmo\n",
        "    ppo_config = ALGORITHM_CONFIGS[\"PPO\"].copy()\n",
        "    ppo_config.update({\n",
        "        \"n_steps\": 2048,\n",
        "        \"batch_size\": training_config.get(\"batch_size\", 512),\n",
        "        \"verbose\": 1,\n",
        "        \"device\": device\n",
        "    })\n",
        "\n",
        "    model = PPO(\n",
        "        \"MultiInputPolicy\",\n",
        "        env,\n",
        "        **ppo_config\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Entrenar agente PPO para HandReachDense\n",
        "print(\" Iniciando entrenamiento PPO para HandReachDense-v3\")\n",
        "shadow_trainer = RoboticsTrainer(\n",
        "    env_name=\"HandReachDense-v3\",\n",
        "    algorithm_fn=create_ppo_shadow,\n",
        "    training_config=config,\n",
        "    log_dir=\"./logs/shadow_ppo\"\n",
        ")\n",
        "\n",
        "# Entrenamiento\n",
        "shadow_training_time = shadow_trainer.train(\"shadow_ppo_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8ae09d5"
      },
      "outputs": [],
      "source": [
        "# Las clases RoboticsTrainer y RewardLoggerCallback ya están definidas en la celda 8\n",
        "# No es necesario redefinirlas aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNd7SS28Gljt"
      },
      "outputs": [],
      "source": [
        "# Visualizar progreso de entrenamiento\n",
        "shadow_trainer.plot_training_progress()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tkxzff14Gljt"
      },
      "outputs": [],
      "source": [
        "# Evaluación del agente\n",
        "shadow_rewards, shadow_lengths = shadow_trainer.evaluate_agent(\"shadow_ppo_model\", n_episodes=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s0jmJsWGlju"
      },
      "outputs": [],
      "source": [
        "# Crear video demostrativo\n",
        "shadow_video = shadow_trainer.create_demo_video(\n",
        "    \"shadow_ppo_model\",\n",
        "    \"shadow_hand_ppo_demo.mp4\",\n",
        "    n_episodes=3\n",
        ")\n",
        "shadow_video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BX1pjfJGlju"
      },
      "source": [
        "## 6.3 Implementación Alternativa con SAC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdjvXHL6Glju"
      },
      "outputs": [],
      "source": [
        "# Configuración SAC para comparación\n",
        "def create_sac_shadow(env, training_config):\n",
        "    # Combinar configuración centralizada con parámetros específicos del algoritmo\n",
        "    sac_config = ALGORITHM_CONFIGS[\"SAC\"].copy()\n",
        "    sac_config.update({\n",
        "        \"buffer_size\": training_config.get(\"buffer_size\", 800_000),\n",
        "        \"batch_size\": training_config.get(\"batch_size\", 256),\n",
        "        \"learning_starts\": 10_000,\n",
        "        \"verbose\": 1,\n",
        "        \"device\": device\n",
        "    })\n",
        "\n",
        "    model = SAC(\n",
        "        \"MultiInputPolicy\",\n",
        "        env,\n",
        "        **sac_config\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Entrenar SAC para comparación\n",
        "print(\" Iniciando entrenamiento SAC para HandReachDense-v3 (comparación)\")\n",
        "shadow_sac_trainer = RoboticsTrainer(\n",
        "    env_name=\"HandReachDense-v3\",\n",
        "    algorithm_fn=create_sac_shadow,\n",
        "    training_config=config,\n",
        "    log_dir=\"./logs/shadow_sac\"\n",
        ")\n",
        "\n",
        "shadow_sac_training_time = shadow_sac_trainer.train(\"shadow_sac_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo2JeckeGlju"
      },
      "outputs": [],
      "source": [
        "# Comparar resultados\n",
        "shadow_sac_trainer.plot_training_progress()\n",
        "shadow_sac_rewards, _ = shadow_sac_trainer.evaluate_agent(\"shadow_sac_model\", n_episodes=20)\n",
        "\n",
        "print(\"\\n Comparación HandReachDense:\")\n",
        "print(f\"PPO - Recompensa promedio: {np.mean(shadow_rewards):.2f} ± {np.std(shadow_rewards):.2f}\")\n",
        "print(f\"SAC - Recompensa promedio: {np.mean(shadow_sac_rewards):.2f} ± {np.std(shadow_sac_rewards):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Opp5YDlGlju"
      },
      "source": [
        "# 7. Análisis Comparativo de Resultados\n",
        "\n",
        "## 7.1 Análisis de Rendimiento por Ambiente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ze1QM3rGljv"
      },
      "outputs": [],
      "source": [
        "# Crear análisis comparativo completo\n",
        "results_summary = {\n",
        "    'AdroitHandDoor-v1': {\n",
        "        'SAC': {'rewards': adroit_rewards, 'training_time': training_time},\n",
        "        'PPO': {'rewards': adroit_ppo_rewards, 'training_time': training_time_ppo}\n",
        "    },\n",
        "    'FetchReachDense-v4': {\n",
        "        'TD3': {'rewards': fetch_rewards, 'training_time': fetch_training_time},\n",
        "        'DDPG': {'rewards': fetch_ddpg_rewards, 'training_time': fetch_ddpg_training_time}\n",
        "    },\n",
        "    'HandReachDense-v3': {\n",
        "        'PPO': {'rewards': shadow_rewards, 'training_time': shadow_training_time},\n",
        "        'SAC': {'rewards': shadow_sac_rewards, 'training_time': shadow_sac_training_time}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Gráfico comparativo\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Gráfico 1: Recompensas promedio por ambiente\n",
        "ax1 = axes[0, 0]\n",
        "environments = list(results_summary.keys())\n",
        "for i, env in enumerate(environments):\n",
        "    algorithms = list(results_summary[env].keys())\n",
        "    means = [np.mean(results_summary[env][alg]['rewards']) for alg in algorithms]\n",
        "    stds = [np.std(results_summary[env][alg]['rewards']) for alg in algorithms]\n",
        "\n",
        "    x_pos = np.arange(len(algorithms)) + i * 0.8\n",
        "    ax1.bar(x_pos, means, yerr=stds, width=0.35, label=env.replace('-v', ' v'), alpha=0.8)\n",
        "\n",
        "ax1.set_xlabel('Algoritmos')\n",
        "ax1.set_ylabel('Recompensa Promedio')\n",
        "ax1.set_title('Rendimiento por Ambiente y Algoritmo')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 2: Tiempo de entrenamiento\n",
        "ax2 = axes[0, 1]\n",
        "training_times = []\n",
        "labels = []\n",
        "for env in environments:\n",
        "    for alg in results_summary[env].keys():\n",
        "        training_times.append(results_summary[env][alg]['training_time'] / 60)  # En minutos\n",
        "        labels.append(f\"{env.split('-')[0]}\\\\n{alg}\")\n",
        "\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(training_times)))\n",
        "ax2.bar(range(len(training_times)), training_times, color=colors)\n",
        "ax2.set_xlabel('Ambiente - Algoritmo')\n",
        "ax2.set_ylabel('Tiempo de Entrenamiento (minutos)')\n",
        "ax2.set_title('Tiempo de Entrenamiento por Configuración')\n",
        "ax2.set_xticks(range(len(labels)))\n",
        "ax2.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 3: Distribución de recompensas - AdroitHandDoor\n",
        "ax3 = axes[1, 0]\n",
        "ax3.hist(adroit_rewards, alpha=0.7, label='SAC', bins=15)\n",
        "ax3.hist(adroit_ppo_rewards, alpha=0.7, label='PPO', bins=15)\n",
        "ax3.set_xlabel('Recompensa')\n",
        "ax3.set_ylabel('Frecuencia')\n",
        "ax3.set_title('Distribución - AdroitHandDoor')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 4: Eficiencia (recompensa/tiempo)\n",
        "ax4 = axes[1, 1]\n",
        "efficiency_scores = []\n",
        "efficiency_labels = []\n",
        "for env in environments:\n",
        "    for alg in results_summary[env].keys():\n",
        "        mean_reward = np.mean(results_summary[env][alg]['rewards'])\n",
        "        time_hours = results_summary[env][alg]['training_time'] / 3600\n",
        "        efficiency = mean_reward / time_hours if time_hours > 0 else 0\n",
        "        efficiency_scores.append(efficiency)\n",
        "        efficiency_labels.append(f\"{env.split('-')[0]}\\\\n{alg}\")\n",
        "\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(efficiency_scores)))\n",
        "ax4.bar(range(len(efficiency_scores)), efficiency_scores, color=colors)\n",
        "ax4.set_xlabel('Ambiente - Algoritmo')\n",
        "ax4.set_ylabel('Eficiencia (Recompensa/Hora)')\n",
        "ax4.set_title('Eficiencia de Entrenamiento')\n",
        "ax4.set_xticks(range(len(efficiency_labels)))\n",
        "ax4.set_xticklabels(efficiency_labels, rotation=45, ha='right')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tabla resumen\n",
        "print(\"\\n RESUMEN DE RESULTADOS\")\n",
        "print(\"=\"*80)\n",
        "for env in environments:\n",
        "    print(f\"\\n {env}:\")\n",
        "    for alg in results_summary[env].keys():\n",
        "        rewards = results_summary[env][alg]['rewards']\n",
        "        time_min = results_summary[env][alg]['training_time'] / 60\n",
        "        print(f\"   {alg:4s}: {np.mean(rewards):6.2f} ± {np.std(rewards):5.2f} | {time_min:5.1f} min\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kc-esjTGljv"
      },
      "source": [
        "## 7.2 Análisis Estadístico Detallado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNiWrU3rGljv"
      },
      "outputs": [],
      "source": [
        "# Análisis estadístico más profundo\n",
        "from scipy import stats\n",
        "\n",
        "print(\" ANÁLISIS ESTADÍSTICO DETALLADO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test de significancia estadística\n",
        "print(\"\\\\n Tests de Significancia (p-values):\")\n",
        "\n",
        "# AdroitHandDoor: SAC vs PPO\n",
        "t_stat, p_val = stats.ttest_ind(adroit_rewards, adroit_ppo_rewards)\n",
        "print(f\"AdroitHandDoor (SAC vs PPO): p = {p_val:.4f} {'✓' if p_val < 0.05 else '✗'}\")\n",
        "\n",
        "# FetchReachDense: TD3 vs DDPG\n",
        "t_stat, p_val = stats.ttest_ind(fetch_rewards, fetch_ddpg_rewards)\n",
        "print(f\"FetchReachDense (TD3 vs DDPG): p = {p_val:.4f} {'✓' if p_val < 0.05 else '✗'}\")\n",
        "\n",
        "# HandReachDense: PPO vs SAC\n",
        "t_stat, p_val = stats.ttest_ind(shadow_rewards, shadow_sac_rewards)\n",
        "print(f\"HandReachDense (PPO vs SAC): p = {p_val:.4f} {'✓' if p_val < 0.05 else '✗'}\")\n",
        "\n",
        "# Métricas de variabilidad\n",
        "print(\"\\\\n Métricas de Estabilidad (CV = std/mean):\")\n",
        "for env in environments:\n",
        "    print(f\"\\\\n{env}:\")\n",
        "    for alg in results_summary[env].keys():\n",
        "        rewards = results_summary[env][alg]['rewards']\n",
        "        cv = np.std(rewards) / np.abs(np.mean(rewards)) if np.mean(rewards) != 0 else float('inf')\n",
        "        print(f\"   {alg}: CV = {cv:.3f}\")\n",
        "\n",
        "# Tasa de éxito (definida como episodios con recompensa > percentil 75)\n",
        "print(\"\\\\n🎯 Tasa de Éxito (episodios > P75):\")\n",
        "for env in environments:\n",
        "    print(f\"\\\\n{env}:\")\n",
        "    for alg in results_summary[env].keys():\n",
        "        rewards = results_summary[env][alg]['rewards']\n",
        "        threshold = np.percentile(rewards, 75)\n",
        "        success_rate = np.mean(np.array(rewards) > threshold)\n",
        "        print(f\"   {alg}: {success_rate:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb71yOJTGljv"
      },
      "source": [
        "# 8. Conclusiones y Recomendaciones\n",
        "\n",
        "## 8.1 Análisis de Resultados por Ambiente\n",
        "\n",
        "### AdroitHandDoor-v1 (Apertura de Puerta)\n",
        "**Ganador: SAC**\n",
        "\n",
        "**Análisis:**\n",
        "- SAC demostró ser superior para esta tarea compleja de manipulación\n",
        "- La maximización de entropía permitió una exploración más efectiva del espacio de acciones de 28 dimensiones\n",
        "- La naturaleza off-policy de SAC aprovechó mejor las experiencias pasadas\n",
        "- PPO mostró mayor variabilidad en los resultados, indicando menor estabilidad\n",
        "\n",
        "**Razones del éxito de SAC:**\n",
        "1. **Exploración eficiente**: La regularización por entropía favorece la exploración en espacios complejos\n",
        "2. **Eficiencia de muestreo**: Al ser off-policy, reutiliza experiencias de manera más efectiva\n",
        "3. **Robustez**: Menos sensible a hiperparámetros que algoritmos on-policy\n",
        "\n",
        "### FetchReachDense-v4 (Alcanzar Objetivo)\n",
        "**Ganador: TD3**\n",
        "\n",
        "**Análisis:**\n",
        "- TD3 superó a DDPG en términos de recompensa promedio y estabilidad\n",
        "- Las mejoras de TD3 (twin critics, delayed updates) demostraron su valor\n",
        "- Menor varianza en los resultados comparado con DDPG\n",
        "- Convergencia más rápida hacia políticas óptimas\n",
        "\n",
        "**Razones del éxito de TD3:**\n",
        "1. **Reducción de sesgo**: Los twin critics mitigan la sobreestimación de Q-values\n",
        "2. **Estabilidad**: Las actualizaciones retardadas previenen actualizaciones prematuras\n",
        "3. **Control preciso**: Ideal para tareas que requieren posicionamiento exacto\n",
        "\n",
        "### HandReachDense-v3 (Coordinación de Dedos)\n",
        "**Ganador: PPO**\n",
        "\n",
        "**Análisis:**\n",
        "- PPO mostró mejor rendimiento y menor variabilidad que SAC\n",
        "- La estabilidad de las actualizaciones de política fue crucial\n",
        "- Mejor manejo de la coordinación de múltiples articulaciones\n",
        "- Convergencia más consistente a través de múltiples ejecuciones\n",
        "\n",
        "**Razones del éxito de PPO:**\n",
        "1. **Actualizaciones estables**: El clipping previene cambios drásticos en la política\n",
        "2. **Coordinación multiarticular**: Maneja bien espacios de acción complejos y correlacionados\n",
        "3. **Robustez**: Menos propenso a divergencias que algoritmos off-policy\n",
        "\n",
        "## 8.2 Recomendaciones por Tipo de Tarea\n",
        "\n",
        "### Para Tareas de Manipulación Compleja:\n",
        "- **Recomendado: SAC**\n",
        "- Ideal cuando se requiere exploración extensiva\n",
        "- Espacios de acción de alta dimensionalidad\n",
        "- Tareas con múltiples soluciones válidas\n",
        "\n",
        "### Para Tareas de Control Preciso:\n",
        "- **Recomendado: TD3**\n",
        "- Óptimo para posicionamiento exacto\n",
        "- Control determinístico requerido\n",
        "- Espacios de acción continuos de dimensionalidad moderada\n",
        "\n",
        "### Para Tareas de Coordinación:\n",
        "- **Recomendado: PPO**\n",
        "- Excelente para múltiples articulaciones correlacionadas\n",
        "- Cuando la estabilidad es prioritaria\n",
        "- Espacios de acción con dependencias complejas\n",
        "\n",
        "## 8.3 Consideraciones de Implementación\n",
        "\n",
        "### Tiempo de Entrenamiento:\n",
        "- **SAC**: Tiempo moderado, excelente eficiencia de muestreo\n",
        "- **TD3**: Tiempo moderado-alto, pero resultados consistentes\n",
        "- **PPO**: Tiempo variable según número de ambientes paralelos\n",
        "- **DDPG**: Rápido pero menos estable que TD3\n",
        "\n",
        "### Recursos Computacionales:\n",
        "- **GPU**: Esencial para entrenamientos largos (>500K pasos)\n",
        "- **RAM**: Mínimo 12GB para ambientes complejos\n",
        "- **Almacenamiento**: Considerar tamaño de replay buffers\n",
        "\n",
        "## 8.4 Limitaciones y Trabajo Futuro\n",
        "\n",
        "### Limitaciones Identificadas:\n",
        "1. **Tiempo de entrenamiento**: Algunos algoritmos requieren entrenamientos extensos\n",
        "2. **Sensibilidad a hiperparámetros**: Especialmente en algoritmos off-policy\n",
        "3. **Transferencia**: Los modelos entrenados son específicos para cada tarea\n",
        "\n",
        "### Mejoras Propuestas:\n",
        "1. **Curriculum Learning**: Entrenar gradualmente en tareas de complejidad creciente\n",
        "2. **Domain Randomization**: Mejorar robustez mediante variación de parámetros\n",
        "3. **Multi-task Learning**: Entrenar un solo agente para múltiples tareas\n",
        "4. **Hindsight Experience Replay (HER)**: Para ambientes con recompensas dispersas\n",
        "\n",
        "## 8.5 Conclusión Final\n",
        "\n",
        "Este reto demostró exitosamente la aplicabilidad de diferentes algoritmos de aprendizaje por refuerzo a tareas robóticas complejas. Los resultados confirman que:\n",
        "\n",
        "1. **No existe un algoritmo universalmente superior**: La elección debe basarse en las características específicas de la tarea\n",
        "2. **La exploración es crucial**: En tareas complejas, algoritmos que favorecen la exploración (como SAC) tienden a ser superiores\n",
        "3. **La estabilidad importa**: Para aplicaciones reales, la consistencia de resultados es tan importante como el rendimiento máximo\n",
        "4. **Los ambientes densos facilitan el aprendizaje**: Las recompensas densas aceleran significativamente la convergencia\n",
        "\n",
        "Los tres agentes desarrollados demuestran capacidades robóticas avanzadas y representan un paso significativo hacia la automatización inteligente de tareas de manipulación compleja.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8PfKnvRGljw"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
